{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU vs CPU Benchmarking: CCC Coefficient Computation\n",
    "\n",
    "This notebook benchmarks the GPU-accelerated CCC (Clustermatch Correlation Coefficient) implementation against the CPU version.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The CCC coefficient measures relationships between features in high-dimensional data. This notebook:\n",
    "- Runs configurable benchmarks comparing GPU (`ccc_gpu`) vs CPU (`ccc`) implementations\n",
    "- Tests various data dimensions (n_features × n_samples)\n",
    "- Validates numerical correctness (GPU results ≈ CPU results)\n",
    "- Visualizes speedup gains from GPU acceleration\n",
    "\n",
    "## Test Configuration\n",
    "\n",
    "Benchmarks test random numerical data across different scales. For a dataset with F features and N samples:\n",
    "- Input shape: (F, N)\n",
    "- Output: F×(F-1)/2 pairwise correlation coefficients\n",
    "- Computational complexity: O(F²×N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "import warnings\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# CCC implementations\n",
    "from ccc.coef.impl_gpu import ccc as ccc_gpu\n",
    "from ccc.coef.impl import ccc\n",
    "\n",
    "# GPU memory management\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    warnings.warn(\"CuPy not available. GPU benchmarks will fail.\")\n",
    "\n",
    "# Styling\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"✓ GPU Available: {GPU_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Modify these parameters to customize the benchmark suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark configuration\n",
    "CONFIG = {\n",
    "    # Test cases: (n_features, n_samples)\n",
    "    # Note: n_samples is fixed at 1000 for consistent comparison\n",
    "    'test_cases': [\n",
    "        # Small cases - quick validation\n",
    "        (10, 100),\n",
    "        (50, 500),\n",
    "        \n",
    "        # Standard benchmark cases (n_samples = 1000)\n",
    "        (500, 1000),\n",
    "        (1000, 1000),\n",
    "        (2000, 1000),\n",
    "        (4000, 1000),\n",
    "        (6000, 1000),\n",
    "        (8000, 1000),\n",
    "        (10000, 1000),\n",
    "        \n",
    "        # Large cases - uncomment for stress testing\n",
    "        (16000, 1000),\n",
    "        (20000, 1000),\n",
    "    ],\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    'seed': 42,\n",
    "    \n",
    "    # CPU core configurations to test (list of values)\n",
    "    # Default: [6, 12, 24] to compare different parallelization levels\n",
    "    'n_cpu_cores': [6, 12, 24],\n",
    "    \n",
    "    # Add singleton features (constant values)\n",
    "    'contain_singletons': False,\n",
    "    \n",
    "    # Numerical tolerance for validation\n",
    "    'rtol': 1e-6,\n",
    "    'atol': 1e-6,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Test cases: {len(CONFIG['test_cases'])}\")\n",
    "print(f\"  CPU core configurations: {CONFIG['n_cpu_cores']}\")\n",
    "print(f\"  Total benchmarks: {len(CONFIG['test_cases']) * len(CONFIG['n_cpu_cores'])}\")\n",
    "print(f\"  Seed: {CONFIG['seed']}\")\n",
    "print(f\"  Contain singletons: {CONFIG['contain_singletons']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gpu_memory():\n",
    "    \"\"\"Clean GPU memory pools to prevent memory leaks between benchmarks.\"\"\"\n",
    "    if GPU_AVAILABLE:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "\n",
    "\n",
    "def generate_test_data(\n",
    "    n_features: int,\n",
    "    n_samples: int,\n",
    "    seed: int,\n",
    "    contain_singletons: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate random test data for benchmarking.\n",
    "    \n",
    "    Args:\n",
    "        n_features: Number of features (rows)\n",
    "        n_samples: Number of samples (columns)\n",
    "        seed: Random seed for reproducibility\n",
    "        contain_singletons: If True, set first row to constant values\n",
    "    \n",
    "    Returns:\n",
    "        Random data array of shape (n_features, n_samples)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    data = np.random.rand(n_features, n_samples)\n",
    "    \n",
    "    if contain_singletons:\n",
    "        data[0, :] = 0.0  # First feature is constant\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def validate_results(\n",
    "    gpu_results: np.ndarray,\n",
    "    cpu_results: np.ndarray,\n",
    "    rtol: float = 1e-6,\n",
    "    atol: float = 1e-6\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate that GPU and CPU results match within tolerance.\n",
    "    \n",
    "    Args:\n",
    "        gpu_results: Coefficients from GPU implementation\n",
    "        cpu_results: Coefficients from CPU implementation\n",
    "        rtol: Relative tolerance\n",
    "        atol: Absolute tolerance\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with validation metrics\n",
    "    \"\"\"\n",
    "    # Check if results are close\n",
    "    close_mask = np.isclose(gpu_results, cpu_results, rtol=rtol, atol=atol)\n",
    "    not_close = np.sum(~close_mask)\n",
    "    total = len(gpu_results)\n",
    "    \n",
    "    # Calculate differences\n",
    "    diff = np.abs(gpu_results - cpu_results)\n",
    "    max_diff = np.max(diff) if len(diff) > 0 else 0.0\n",
    "    \n",
    "    validation = {\n",
    "        'all_close': not_close == 0,\n",
    "        'not_close_count': not_close,\n",
    "        'not_close_percentage': 100.0 * not_close / total if total > 0 else 0.0,\n",
    "        'max_diff': max_diff,\n",
    "        'total_coefficients': total,\n",
    "    }\n",
    "    \n",
    "    return validation\n",
    "\n",
    "\n",
    "def format_time(seconds: float) -> str:\n",
    "    \"\"\"Format time in human-readable format.\"\"\"\n",
    "    if seconds < 1:\n",
    "        return f\"{seconds*1000:.2f} ms\"\n",
    "    elif seconds < 60:\n",
    "        return f\"{seconds:.2f} s\"\n",
    "    else:\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = seconds % 60\n",
    "        return f\"{minutes}m {secs:.2f}s\"\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Benchmarks\n",
    "\n",
    "Execute benchmarks for all configured test cases. This may take several minutes depending on the test sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Calculate total number of benchmarks\n",
    "total_benchmarks = len(CONFIG['test_cases']) * len(CONFIG['n_cpu_cores'])\n",
    "current_benchmark = 0\n",
    "\n",
    "print(f\"Running {total_benchmarks} benchmark(s)...\")\n",
    "print(f\"Test cases: {len(CONFIG['test_cases'])}, CPU configurations: {len(CONFIG['n_cpu_cores'])}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for n_cpu_cores in CONFIG['n_cpu_cores']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CPU CONFIGURATION: {n_cpu_cores} cores\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for i, (n_features, n_samples) in enumerate(CONFIG['test_cases'], 1):\n",
    "        current_benchmark += 1\n",
    "        print(f\"\\n[{current_benchmark}/{total_benchmarks}] Testing: {n_features} features × {n_samples} samples with {n_cpu_cores} cores\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Generate test data\n",
    "        data = generate_test_data(\n",
    "            n_features=n_features,\n",
    "            n_samples=n_samples,\n",
    "            seed=CONFIG['seed'],\n",
    "            contain_singletons=CONFIG['contain_singletons']\n",
    "        )\n",
    "        \n",
    "        # Calculate expected number of coefficients\n",
    "        n_coefficients = n_features * (n_features - 1) // 2\n",
    "        print(f\"  Data shape: {data.shape}\")\n",
    "        print(f\"  Expected coefficients: {n_coefficients:,}\")\n",
    "        \n",
    "        try:\n",
    "            # GPU benchmark\n",
    "            clean_gpu_memory()\n",
    "            print(f\"  Running GPU version...\", end=' ', flush=True)\n",
    "            start_gpu = time.time()\n",
    "            gpu_results = ccc_gpu(data)\n",
    "            gpu_time = time.time() - start_gpu\n",
    "            print(f\"✓ {format_time(gpu_time)}\")\n",
    "            \n",
    "            # CPU benchmark\n",
    "            print(f\"  Running CPU version ({n_cpu_cores} cores)...\", end=' ', flush=True)\n",
    "            start_cpu = time.time()\n",
    "            cpu_results = ccc(data, n_jobs=n_cpu_cores)\n",
    "            cpu_time = time.time() - start_cpu\n",
    "            print(f\"✓ {format_time(cpu_time)}\")\n",
    "            \n",
    "            # Calculate speedup\n",
    "            speedup = cpu_time / gpu_time if gpu_time > 0 else 0\n",
    "            \n",
    "            # Validate correctness\n",
    "            validation = validate_results(\n",
    "                gpu_results,\n",
    "                cpu_results,\n",
    "                rtol=CONFIG['rtol'],\n",
    "                atol=CONFIG['atol']\n",
    "            )\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\n  Results:\")\n",
    "            print(f\"    Speedup: {speedup:.2f}x\")\n",
    "            print(f\"    Validation: {'PASS' if validation['all_close'] else 'FAIL'}\")\n",
    "            if not validation['all_close']:\n",
    "                print(f\"    Not close: {validation['not_close_count']} ({validation['not_close_percentage']:.2f}%)\")\n",
    "                print(f\"    Max difference: {validation['max_diff']:.2e}\")\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'n_features': n_features,\n",
    "                'n_samples': n_samples,\n",
    "                'n_cpu_cores': n_cpu_cores,\n",
    "                'n_coefficients': n_coefficients,\n",
    "                'gpu_time': gpu_time,\n",
    "                'cpu_time': cpu_time,\n",
    "                'speedup': speedup,\n",
    "                'validation_passed': validation['all_close'],\n",
    "                'max_diff': validation['max_diff'],\n",
    "            })\n",
    "            \n",
    "            # Clean up\n",
    "            clean_gpu_memory()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ERROR: {str(e)}\")\n",
    "            print(f\"  Skipping this test case.\")\n",
    "            continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"\\n✓ Benchmarking complete! {len(results)}/{total_benchmarks} tests successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "if len(df_results) == 0:\n",
    "    print(\"No results to display. Please check for errors above.\")\n",
    "else:\n",
    "    # Format for display\n",
    "    df_display = df_results.copy()\n",
    "    df_display['gpu_time_formatted'] = df_display['gpu_time'].apply(format_time)\n",
    "    df_display['cpu_time_formatted'] = df_display['cpu_time'].apply(format_time)\n",
    "    df_display['speedup_formatted'] = df_display['speedup'].apply(lambda x: f\"{x:.2f}x\")\n",
    "    \n",
    "    # Display table\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    print(\"=\" * 120)\n",
    "    display_cols = [\n",
    "        'n_cpu_cores', 'n_features', 'n_samples', 'n_coefficients',\n",
    "        'gpu_time_formatted', 'cpu_time_formatted', 'speedup_formatted',\n",
    "        'validation_passed'\n",
    "    ]\n",
    "    print(df_display[display_cols].to_string(index=False))\n",
    "    \n",
    "    # Summary statistics by CPU core count\n",
    "    print(\"\\n\" + \"=\" * 120)\n",
    "    print(\"\\nSummary Statistics by CPU Core Configuration:\")\n",
    "    print(\"-\" * 120)\n",
    "    for n_cores in sorted(df_results['n_cpu_cores'].unique()):\n",
    "        subset = df_results[df_results['n_cpu_cores'] == n_cores]\n",
    "        print(f\"\\n{n_cores} CPU cores:\")\n",
    "        print(f\"  Mean speedup:   {subset['speedup'].mean():.2f}x\")\n",
    "        print(f\"  Median speedup: {subset['speedup'].median():.2f}x\")\n",
    "        print(f\"  Min speedup:    {subset['speedup'].min():.2f}x\")\n",
    "        print(f\"  Max speedup:    {subset['speedup'].max():.2f}x\")\n",
    "        print(f\"  All validations passed: {subset['validation_passed'].all()}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(\"\\n\" + \"=\" * 120)\n",
    "    print(\"\\nOverall Statistics:\")\n",
    "    print(f\"  Mean speedup: {df_results['speedup'].mean():.2f}x\")\n",
    "    print(f\"  Median speedup: {df_results['speedup'].median():.2f}x\")\n",
    "    print(f\"  Min speedup: {df_results['speedup'].min():.2f}x\")\n",
    "    print(f\"  Max speedup: {df_results['speedup'].max():.2f}x\")\n",
    "    print(f\"  All validations passed: {df_results['validation_passed'].all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization: GPU Speedup Across CPU Configurations\n",
    "\n",
    "This plot shows how GPU speedup varies with the number of features for different CPU parallelization levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_results) == 0:\n",
    "    print(\"No results to visualize.\")\n",
    "else:\n",
    "    # Create publication-ready figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 7))\n",
    "    \n",
    "    # Define line styles and markers for each CPU configuration\n",
    "    styles = {\n",
    "        6: {'color': '#1f77b4', 'marker': 'o', 'linestyle': '-', 'label': '6 cores'},\n",
    "        12: {'color': '#ff7f0e', 'marker': 's', 'linestyle': '-', 'label': '12 cores'},\n",
    "        24: {'color': '#2ca02c', 'marker': '^', 'linestyle': '-', 'label': '24 cores'},\n",
    "    }\n",
    "    \n",
    "    # Plot speedup for each CPU configuration\n",
    "    for n_cores in sorted(df_results['n_cpu_cores'].unique()):\n",
    "        subset = df_results[df_results['n_cpu_cores'] == n_cores].sort_values('n_features')\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            style = styles.get(n_cores, {'color': 'gray', 'marker': 'o', 'linestyle': '-', 'label': f'{n_cores} cores'})\n",
    "            ax.plot(subset['n_features'], subset['speedup'],\n",
    "                   color=style['color'],\n",
    "                   marker=style['marker'],\n",
    "                   linestyle=style['linestyle'],\n",
    "                   linewidth=2.5,\n",
    "                   markersize=8,\n",
    "                   label=style['label'],\n",
    "                   markeredgewidth=1.5,\n",
    "                   markeredgecolor='white')\n",
    "    \n",
    "    # Add reference line for no speedup\n",
    "    ax.axhline(y=1, color='red', linestyle='--', linewidth=1.5, alpha=0.6, label='No speedup (1x)')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Number of Features (genes)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Speedup (GPU vs CPU)', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('GPU Performance Speedup Across Different CPU Parallelization Levels', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    \n",
    "    # Grid\n",
    "    ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Legend\n",
    "    ax.legend(loc='best', fontsize=11, framealpha=0.95, edgecolor='gray')\n",
    "    \n",
    "    # Improve tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "    \n",
    "    # Set y-axis to start from 0 or slightly below minimum speedup\n",
    "    y_min = max(0, df_results['speedup'].min() - 2)\n",
    "    y_max = df_results['speedup'].max() * 1.1\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Speedup visualization generated successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for saving\n",
    "SAVE_RESULTS = False  # Set to True to save results\n",
    "OUTPUT_DIR = Path('results')\n",
    "\n",
    "if SAVE_RESULTS and len(df_results) > 0:\n",
    "    # Create output directory\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save results to CSV\n",
    "    timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "    csv_path = OUTPUT_DIR / f'benchmark_results_{timestamp}.csv'\n",
    "    df_results.to_csv(csv_path, index=False)\n",
    "    print(f\"✓ Results saved to: {csv_path}\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config_path = OUTPUT_DIR / f'benchmark_config_{timestamp}.txt'\n",
    "    with open(config_path, 'w') as f:\n",
    "        f.write(\"Benchmark Configuration\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        for key, value in CONFIG.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    print(f\"✓ Configuration saved to: {config_path}\")\n",
    "    \n",
    "    print(f\"\\n✓ All results saved to directory: {OUTPUT_DIR.absolute()}\")\n",
    "else:\n",
    "    if len(df_results) == 0:\n",
    "        print(\"No results to save.\")\n",
    "    else:\n",
    "        print(\"Set SAVE_RESULTS = True to export results and figures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results (Optional)\n",
    "\n",
    "Export results and figures for use in publications or reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides an interactive benchmarking framework for comparing GPU and CPU implementations of the CCC coefficient computation across different CPU parallelization levels.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on the benchmark results:\n",
    "- GPU acceleration provides significant speedups, especially for larger datasets\n",
    "- Speedup varies with the number of CPU cores used in the baseline\n",
    "- Results are numerically equivalent between GPU and CPU implementations (within tolerance)\n",
    "- Performance gains scale with problem size (number of features)\n",
    "\n",
    "### Configuration Options\n",
    "\n",
    "To customize the analysis:\n",
    "1. **Modify test cases**: Edit `CONFIG['test_cases']` to add/remove data dimensions\n",
    "   - Standard format: (n_features, n_samples)\n",
    "   - Recommended: Keep n_samples=1000 for consistent comparison\n",
    "   \n",
    "2. **Change CPU configurations**: Edit `CONFIG['n_cpu_cores']` list\n",
    "   - Default: [6, 12, 24]\n",
    "   - Add more values to test different parallelization levels\n",
    "   \n",
    "3. **Test singletons**: Set `CONFIG['contain_singletons']=True` to include constant features\n",
    "\n",
    "4. **Save results**: Set `SAVE_RESULTS=True` in Section 7 to export data\n",
    "\n",
    "### Expected Behavior\n",
    "\n",
    "- **Small datasets (< 100 features)**: GPU may be slower due to overhead\n",
    "- **Medium datasets (100-1000 features)**: GPU starts showing significant speedup\n",
    "- **Large datasets (> 1000 features)**: GPU provides substantial performance gains\n",
    "\n",
    "### References\n",
    "\n",
    "- Original test file: `tests/gpu/test_ccc_gpu.py`\n",
    "- GPU implementation: `libs/ccc/coef/impl_gpu.py`\n",
    "- CPU implementation: `libs/ccc/coef/impl.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
